{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66bc991",
   "metadata": {},
   "source": [
    "# FROMAGe Contextual Image Retrieval\n",
    "\n",
    "This is a notebook showcasing the contextual image retrieval results from our paper, [Grounding Language Models to Images for Multimodal Generation](https://arxiv.org/abs/2301.13823). This result is reported in Table 1. The results of this notebook may be slightly different from the paper, as the Flickr images from Visual Storytelling may disappear over time.\n",
    "\n",
    "At least 18GB of GPU memory is required to run FROMAGe, and it has only been tested on A6000, V100, and 3090 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475add8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import logging\n",
    "from tqdm import notebook\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fromage import models\n",
    "from fromage import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e884127",
   "metadata": {},
   "source": [
    "### Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4646a124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuggingFace AutoFeatureExtractor for openai/clip-vit-large-patch14.\n",
      "Using facebook/opt-6.7b for the language model.\n",
      "Using openai/clip-vit-large-patch14 for the visual model with 1 visual tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb4489d7d8b4235abe3f965318af27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the LM.\n",
      "Initializing embedding for the retrieval token [RET] (id = 50266).\n",
      "Restoring pretrained weights for the visual model.\n",
      "Freezing the VM.\n"
     ]
    }
   ],
   "source": [
    "# Load model used in the paper.\n",
    "model_dir = './fromage_model/'\n",
    "model = models.load_fromage(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d373b",
   "metadata": {},
   "source": [
    "### Contextual Image Retrieval for Visual Storytelling\n",
    "\n",
    "Download the Visual Storytelling SIS dataset from [their website](https://visionandlanguage.net/VIST/json_files/story-in-sequence/SIS-with-labels.tar.gz). Extract the files (there should be three sets: train, val, and test). We'll use the val set for reporting results.\n",
    "\n",
    "First, we'll do some data preprocessing to make things easier for us later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf39013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8034\n"
     ]
    }
   ],
   "source": [
    "vist_val_json_path = 'sis/val.story-in-sequence.json'\n",
    "with open(vist_val_json_path, 'r') as f:\n",
    "    vist_data_raw = json.load(f)\n",
    "    \n",
    "# Format into a dictionary of {story_id: data} items.\n",
    "vist_data = {\n",
    "    'annotations': collections.defaultdict(list)\n",
    "}\n",
    "used_image_ids = []\n",
    "\n",
    "\n",
    "for ann in vist_data_raw['annotations']:\n",
    "    assert len(ann) == 1\n",
    "    ann = ann[0]\n",
    "    story_id = ann['story_id']\n",
    "    vist_data['annotations'][story_id].append({\n",
    "        'caption': ann['text'],\n",
    "        'image_id': ann['photo_flickr_id'],\n",
    "        'sequence_index': ann['worker_arranged_photo_order'],\n",
    "    })\n",
    "    used_image_ids.append(ann['photo_flickr_id'])\n",
    "\n",
    "used_image_ids = set(used_image_ids)\n",
    "print(len(used_image_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8b664",
   "metadata": {},
   "source": [
    "Then, we can precompute features for all images. This will be used for image retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17d5cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2url = {}\n",
    "\n",
    "for image_data in vist_data_raw['images']:\n",
    "    image_id = image_data['id']\n",
    "    if image_id in used_image_ids:\n",
    "        image_url = image_data.get('url_o', None)\n",
    "        if image_url is not None:\n",
    "            id2url[image_id] = image_url\n",
    "\n",
    "# Extract image features.\n",
    "embs_fn = 'sis_img_features.npy'\n",
    "\n",
    "# Compute visual embeddings.\n",
    "if not os.path.exists(embs_fn):\n",
    "    print(f'{embs_fn} does not exist, computing it from scratch.')\n",
    "    all_visual_embs = []\n",
    "    all_image_ids = []\n",
    "\n",
    "    for image_id, image_url in notebook.tqdm(id2url.items()):\n",
    "        try:\n",
    "            images = utils.get_image_from_url(image_url)\n",
    "            pixel_values = utils.get_pixel_values_for_model(model.model.feature_extractor, images)\n",
    "            pixel_values = pixel_values.to(device=model.model.logit_scale.device, dtype=model.model.logit_scale.dtype)\n",
    "            pixel_values = pixel_values[None, ...]\n",
    "            visual_embs = model.model.get_visual_embs(pixel_values, mode='retrieval')\n",
    "            all_visual_embs.append(visual_embs.float().cpu().detach().numpy())\n",
    "            all_image_ids.append(image_id)\n",
    "        except Image.UnidentifiedImageError:\n",
    "            pass\n",
    "\n",
    "    all_image_ids = np.array(all_image_ids)\n",
    "    all_visual_embs = np.concatenate(all_visual_embs, axis=0)\n",
    "    assert all_image_ids.shape[0] == all_visual_embs.shape[0], (all_image_ids.shape, all_visual_embs.shape)\n",
    "    print(all_image_ids.shape, all_visual_embs.shape)\n",
    "\n",
    "    with open(embs_fn, 'wb') as wf:\n",
    "        np.save(wf, {'image_ids': all_image_ids, 'embeddings': all_visual_embs})\n",
    "\n",
    "# Load embeddings.\n",
    "with open(embs_fn, 'rb') as wf:\n",
    "    embs_data = np.load(wf, allow_pickle=True).item()\n",
    "    all_image_ids = embs_data['image_ids']\n",
    "    emb_matrix = embs_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970a5d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_matrix.shape torch.Size([7043, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "len(all_image_ids), emb_matrix.shape\n",
    "\n",
    "# Normalize embedding matrix to be suitable for image retrieval.\n",
    "logit_scale = model.model.logit_scale.exp()\n",
    "emb_matrix = torch.tensor(emb_matrix, dtype=logit_scale.dtype).to(logit_scale.device)\n",
    "emb_matrix = emb_matrix / emb_matrix.norm(dim=-1, keepdim=True)\n",
    "emb_matrix = logit_scale * emb_matrix\n",
    "print('emb_matrix.shape', emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944691a6",
   "metadata": {},
   "source": [
    "Then, for each VIST example, we process it as `<caption1><img1><caption2><img2>...<caption5> [RET]`, providing this as input to FROMAGe, and retrieve the image corresponding to the `[RET]` embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d20c3c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e320375086411eaf5e0ca0acc6f6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topk = (1, 5, 10)\n",
    "top_k_preds = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for story_idx, (story_id, story_data) in notebook.tqdm(enumerate(vist_data['annotations'].items()), total=len(vist_data['annotations'])):\n",
    "        gt_image_id = story_data[-1]['image_id']\n",
    "        skip = False  # Skip examples that do not have images (due to URLs being taken down, or something)\n",
    "        for s in story_data:\n",
    "            if s['image_id'] not in all_image_ids or s['image_id'] not in id2url:\n",
    "                skip = True\n",
    "                break\n",
    "\n",
    "        if not skip:\n",
    "            # Use the first n-1 images and n captions as input.\n",
    "            image_urls = [id2url[s['image_id']] for s in story_data[:-1]]\n",
    "            captions = [s['caption'] for s in story_data]\n",
    "            assert len(image_urls) == len(captions) - 1\n",
    "\n",
    "            visual_embs = []\n",
    "            # Compute embeddings for the input images.\n",
    "            images = [utils.get_image_from_url(image_url) for image_url in image_urls]\n",
    "            pixel_values = [utils.get_pixel_values_for_model(model.model.feature_extractor, image) for image in images]\n",
    "            pixel_values = torch.stack(pixel_values, dim=0)  # (n-1, 3, 224, 224)\n",
    "            pixel_values = pixel_values.to(device=model.model.logit_scale.device, dtype=model.model.logit_scale.dtype)\n",
    "            visual_embs = model.model.get_visual_embs(pixel_values, mode='captioning')\n",
    "\n",
    "            # Compute embeddings for the input captions.\n",
    "            all_input_ids = []\n",
    "            for i, c in enumerate(captions):\n",
    "                if i == len(captions) - 1:\n",
    "                    c += '[RET]'  # Add the [RET] token to the final caption.\n",
    "                input_ids = model.model.tokenizer(c, add_special_tokens=True, return_tensors=\"pt\").input_ids.to(emb_matrix.device)\n",
    "                all_input_ids.append(input_ids)\n",
    "            \n",
    "            input_embs = [model.model.input_embeddings(s)[0, ...] for s in all_input_ids]  # (N, T, D)\n",
    "\n",
    "            # Interleave captions and images as [caption1, image1, caption2, ..., image4, caption5].\n",
    "            final_input_embs = []\n",
    "            assert len(visual_embs) == len(input_embs) - 1\n",
    "            for i in range(len(images)):\n",
    "                final_input_embs.append(input_embs[i])\n",
    "                final_input_embs.append(visual_embs[i])\n",
    "            final_input_embs.append(input_embs[len(images)])\n",
    "            final_input_embs = torch.cat(final_input_embs, dim=0)[None, ...]  # (1, T, 4096)\n",
    "            \n",
    "            # Get embedding of the [RET] token, and compute scores:\n",
    "            output = model.model.lm(inputs_embeds=final_input_embs, labels=None, use_cache=False, output_hidden_states=True)\n",
    "            last_hidden_state = model.model.text_hidden_fcs[0](output.hidden_states[-1])\n",
    "            ret_emb = last_hidden_state[:, -1, :]\n",
    "\n",
    "            ret_emb = ret_emb / ret_emb.norm(dim=1, keepdim=True)\n",
    "            scores = ret_emb.squeeze() @ emb_matrix.squeeze().T\n",
    "            \n",
    "            # Don't retrieve previously seen images.\n",
    "            prev_image_ids = [s['image_id'] for s in story_data[:-1]]\n",
    "            for prev_id in prev_image_ids:\n",
    "                scores[np.where(all_image_ids == prev_id)[0]] -= 10000\n",
    "            \n",
    "            # Store top-k preds.\n",
    "            _, preds = scores.topk(max(topk))\n",
    "            preds = preds.cpu().detach().numpy()\n",
    "            preds = [all_image_ids[p] for p in preds]\n",
    "            top_k_preds[story_id] = {'topk_preds': preds, 'gt': gt_image_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2a81a",
   "metadata": {},
   "source": [
    "Finally, we can compute Recall@k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7686317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, acc=0.18232\n",
      "k=5, acc=0.42682\n",
      "k=10, acc=0.51775\n"
     ]
    }
   ],
   "source": [
    "top_k_accuracy = collections.defaultdict(list)\n",
    "\n",
    "for story_id, results in top_k_preds.items():\n",
    "    for k in topk:\n",
    "        acc = results['gt'] in results['topk_preds'][:k]\n",
    "        top_k_accuracy[k].append(acc)\n",
    "\n",
    "for k in topk:\n",
    "    result_str = f'k={k}, acc={np.mean(top_k_accuracy[k]):.5f}'\n",
    "    print(result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fd749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
