{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66bc991",
   "metadata": {},
   "source": [
    "# FROMAGe Visual Dialog (Image Retrieval)\n",
    "\n",
    "This is a notebook reproducing the VisDial text-to-image (T2I) retrieval results from our paper, [Grounding Language Models to Images for Multimodal Inputs and Outputs](https://arxiv.org/abs/2301.13823). This result is reported in Table 2 of the paper. This measures the recall of the model in selecting the appropriate image conditioned on a dialogue sequence.\n",
    "\n",
    "At least 18GB of GPU memory is required to run FROMAGe, and it has only been tested on A6000, V100, and 3090 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475add8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import logging\n",
    "from tqdm import notebook\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fromage import models\n",
    "from fromage import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e884127",
   "metadata": {},
   "source": [
    "### Load Pretrained FROMAGe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4646a124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuggingFace AutoFeatureExtractor for openai/clip-vit-large-patch14.\n",
      "Using facebook/opt-6.7b for the language model.\n",
      "Using openai/clip-vit-large-patch14 for the visual model with 1 visual tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aa1cfecea64ed6a4d2c3cbb946d463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the LM.\n",
      "Initializing embedding for the retrieval token [RET] (id = 50266).\n",
      "Restoring pretrained weights for the visual model.\n",
      "Freezing the VM.\n"
     ]
    }
   ],
   "source": [
    "# Load model used in the paper.\n",
    "model_dir = './fromage_model/'\n",
    "model = models.load_fromage(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d373b",
   "metadata": {},
   "source": [
    "### VisDial\n",
    "\n",
    "Download the VisDial validation [annotations](https://www.dropbox.com/s/ibs3a0zhw74zisc/visdial_1.0_val.zip?dl=0), the [dense answer annotations](https://www.dropbox.com/s/3knyk09ko4xekmc/visdial_1.0_val_dense_annotations.json?dl=0) (for computing MRR) and the [images](https://www.dropbox.com/s/twmtutniktom7tu/VisualDialog_val2018.zip?dl=0). Extract everything to the `VisualDialog` folder.\n",
    "\n",
    "First, we'll do some data preprocessing to make things easier for us later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf39013",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'VisualDialog/'\n",
    "split = 'val'\n",
    "img_dir = os.path.join(base_dir, f'VisualDialog_{split}2018')\n",
    "\n",
    "with open(os.path.join(base_dir, f'visdial_1.0_{split}.json'), 'r') as f:\n",
    "    visdial_data = json.load(f)\n",
    "    \n",
    "with open(os.path.join(base_dir, f'visdial_1.0_{split}_dense_annotations.json'), 'r') as f:\n",
    "    dense_data = json.load(f)\n",
    "\n",
    "# Check that dense and sparse data are aligned.\n",
    "assert len(dense_data) == len(visdial_data['data']['dialogs'])\n",
    "for i in range(len(dense_data)):\n",
    "    assert dense_data[i]['image_id'] == visdial_data['data']['dialogs'][i]['image_id']\n",
    "    \n",
    "questions = visdial_data['data']['questions']\n",
    "answers = visdial_data['data']['answers']\n",
    "dialogs = visdial_data['data']['dialogs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418ee205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_values_from_path(path: str, feature_extractor):\n",
    "    \"\"\"Helper function for getting images pixels from a local path.\"\"\"\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    pixel_values = utils.get_pixel_values_for_model(feature_extractor, img)\n",
    "    if torch.cuda.is_available():\n",
    "        pixel_values = pixel_values.bfloat16()\n",
    "        pixel_values = pixel_values.cuda()\n",
    "    return pixel_values[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944691a6",
   "metadata": {},
   "source": [
    "Then, we compute the image features and text features for each VisDial example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20c3c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdb984f282f45babbe00eb49fbb6ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topk = (1, 5, 10)\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='none').cuda()\n",
    "\n",
    "all_visual_embs = []\n",
    "all_text_embs = []\n",
    "\n",
    "for example_idx in notebook.tqdm(range(len(dialogs))):\n",
    "    dialog = dialogs[example_idx]\n",
    "    image_id = str(dialog['image_id']).rjust(12, '0')\n",
    "    contexts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = get_pixel_values_from_path(\n",
    "            os.path.join(img_dir, f'VisualDialog_{split}2018_{image_id}.jpg'),\n",
    "            model.model.feature_extractor)\n",
    "        visual_embs = model.model.get_visual_embs(images, mode='retrieval')\n",
    "\n",
    "        for i in range(len(dialog['dialog'])):\n",
    "            contexts.append('Q: ' + questions[dialog['dialog'][i]['question']] + '?')\n",
    "            contexts.append('A: ' + answers[dialog['dialog'][i]['answer']] + '.')\n",
    "\n",
    "        full_context_sent = ' '.join(contexts) + '[RET]'\n",
    "        input_ids = model.model.tokenizer(full_context_sent, add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "        input_ids = input_ids.cuda()\n",
    "        input_embs = model.model.input_embeddings(input_ids)  # (N, T, D)\n",
    "        generated_ids, output_embs, _ = model(input_embs, None, None, generate=True, num_words=1, temperature=0.0)\n",
    "        embeddings = output_embs[0]\n",
    "\n",
    "        full_input_ids = torch.cat([input_ids, generated_ids], dim=1)\n",
    "        ret_emb = embeddings[:, -1, :]  \n",
    "\n",
    "        all_visual_embs.append(visual_embs.cpu().detach().float().numpy())\n",
    "        all_text_embs.append(ret_emb.cpu().detach().float().numpy())\n",
    "\n",
    "# Compute scores over the whole dataset:\n",
    "scores = np.concatenate(all_visual_embs, axis=0)[:, 0, :] @ np.concatenate(all_text_embs, axis=0).T\n",
    "scores = torch.tensor(scores).float()\n",
    "assert scores.shape == (2064, 2064), scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0649fc",
   "metadata": {},
   "source": [
    "Finally, we can compute the Recall@k scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627e9f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-k, k=1, acc=0.20785\n",
      "top-k, k=5, acc=0.44913\n",
      "top-k, k=10, acc=0.55959\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "_, preds = scores.topk(max(topk))\n",
    "for k in topk:\n",
    "    labels = torch.arange(preds.shape[0])\n",
    "    correct = torch.any(preds[:, :k] == labels[:, None], axis=1).sum()\n",
    "    acc = correct / preds.shape[0]\n",
    "    print(f'top-k, k={k}, acc={acc:.5f}')\n",
    "print('=' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a01a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
